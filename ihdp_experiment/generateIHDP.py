import numpy as np
from numpy.random import choice
from sklearn import datasets, linear_model
import matplotlib.pylot as plt

# Generate semi-synthetic data for Fair Causal Inference according to Madras https://arxiv.org/abs/1809.02519


def GenerateSensitive(x, z):
    """
    Generate sensitive var A (e.g. interpretable as mothers race)
    """
    # ensure some correlation with proxy, and perpendicular to z, relation y ensured by dgp for y
    # x has zero mean and unit variance, from this we want a distribution of a

    # Create coefficients:
    beta = np.random.uniform(0, 1, x.shape[1])
    # create a signs
    p = np.matmul(beta, np.transpose(x))[:, np.newaxis]
    # make approx orthogonal to z
    regr = linear_model.LinearRegression()
    regr.fit(p, z)
    resid = p - regr.coef_*z

    a_gen = np.array((resid > 0)).astype(int)
    return a_gen


def GenerateOutcomes(x, z, num_cont, num_bin):
    """
    Following the generating procedure defined by Madras in Algorithm 2
    """
    # As defined by Madras
    num_z = z.shape[1]
    w = -11
    beta_a = 6

    # Algorithm 2
    # horizontal concatenation
    xz = np.concatenate((x, z), 1)
    W = np.ones(xz.shape[1])*.5

    # lists to store generated values
    y_t0_a0, y_t1_a0, y_t0_a1, y_t1_a1 = list(), list(), list(), list()
    mu_t0_a0, mu_t1_a0, mu_t0_a1, mu_t1_a1 = list(), list(), list(), list()

    # loop over observations because all need individual beta sample
    for obs in xz:
        # sample new beta
        beta_cont = choice([0, .1, .2, .3, .4], num_cont, p=[.5, .125, .125, .125, .125])
        beta_bin = choice([0, .1, .2, .3, .4], num_bin, p=[.6, .1, .1, .1, .1])

        beta_z = choice([.4, .6], num_z, p=[.5, .5])
        # in x, continuous variables come first
        beta = np.concatenate((beta_cont, beta_bin, beta_z), 0)

        # calculate y dist
        mu1 = np.matmul(np.exp(obs + W), beta)
        mu_t0_a0.append(mu1)
        mu2 = np.matmul(obs, beta)-w
        mu_t1_a0.append(mu2)
        mu3 = np.matmul(np.exp(obs + W), beta) + beta_a
        mu_t0_a1.append(mu3)
        mu4 = np.matmul(obs, beta) - w + beta_a
        mu_t1_a1.append(mu4)
        # sample new y
        y_t0_a0.append(np.random.normal(mu1, 1, 1)[0])
        y_t1_a0.append(np.random.normal(mu2, 1, 1)[0])
        y_t0_a1.append(np.random.normal(mu3, 1, 1)[0])
        y_t1_a1.append(np.random.normal(mu4, 1, 1)[0])

    plt_entries = {'y_t0_a0': y_t0_a0, 'y_t1_a0': y_t1_a0, 'y_t0_a1': y_t0_a1, 'y_t1_a1': y_t1_a1}
    plt.figure()
    plt.title('Generated data')

    for label, entry in plt_entries.items():
        plt.hist(entry, label=label, alpha=0.5, bins=20)
    plt.legend()
    plt.show()

    y_all = np.transpose(np.vstack((y_t0_a0, y_t1_a0, y_t0_a1, y_t1_a1)))
    mu_all = np.transpose(np.vstack((mu_t0_a0, mu_t1_a0, mu_t0_a1, mu_t1_a1)))

    # column names should be consistent with above vstack
    y_column = 'y_t0_a0, y_t1_a0, y_t0_a1, y_t1_a1'
    mu_column = 'mu_t0_a0, mu_t1_a0, mu_t0_a1, mu_t1_a1'
    return y_all, mu_all, y_column, mu_column


def GenerateTreatments(z):
    """
    Generate treatment t based on z, defined by Madras in Algorithm 3
    """
    alpha_0, alpha_1, zeta = -0.05, 0.05, 1
    # scale z between 0 and 1:
    z = (z-min(z))/(max(z)-min(z))

    # parameters in work of Madras:
    # alpha_0, alpha_1, zeta = 0.7, 0.4, 1/10

    p0 = np.clip(alpha_0 + (zeta * z), 0, 1)
    p1 = np.clip(alpha_1 + (zeta * z), 0, 1)
    t0 = np.random.binomial(1, p0)
    t1 = np.random.binomial(1, p1)

    t_gen = np.hstack((t0, t1))
    t_column = 't0, t1'

    return t_gen, t_column


if __name__ == "__main__":
    # Generating data 10 times, in order to verify results
    replications = 10
    path_data = "datasets/IHDP/csv"
    # provide indices for binary x features
    binfeats = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
    # which features are continuous
    contfeats = [i for i in range(25) if i not in binfeats]

    # Set index for unobserved confounders
    z_idx = [0]  # continuous variable which is maximally explained by the other variables
    # remove from original feature index
    n_binfeats = len([e for e in binfeats if e not in z_idx])
    n_contfeats = len([e for e in contfeats if e not in z_idx])

    # load proxy data, first 5 columns are generated by Louizos, not used
    x = np.loadtxt(path_data + '/ihdp_npci_1.csv', delimiter=',')[:, 5:]

    # get unobserved confounders from data
    z = x[:, z_idx].copy()
    x = np.delete(x, z_idx, 1)
    # feature indices no longer correct, del to prevent usage
    del binfeats, contfeats

    # normalize all used x (including binary), as needed for DGP Madras
    xm, xs = np.mean(x, 0), np.std(x, 0)
    zm, zs = np.mean(z, 0), np.std(z, 0)
    x_norm = (x - xm) / xs
    z_norm = (z - zm) / zs

    # Generate sensitive variable a, outside repetition loop -> same for each replication
    a = GenerateSensitive(x_norm, z)
    for i in range(replications):

        # generate outcomes for different interventions
        y, mu, y_column, mu_column = GenerateOutcomes(x_norm, z_norm, n_contfeats, n_binfeats)
        t, t_column = GenerateTreatments(z_norm)
        columns = "a, " + t_column + ", " + y_column + ", " + mu_column + ", z"*z_norm.shape[1] + ", x1-xN"

        # Save NOT normalized proxy and confounder values
        data = np.concatenate((a, t, y, mu, z, x), 1)
        filename = "datasets/IHDP_sens/csv/ihdp_sens_" + str(i+1) + ".csv"
        np.savetxt(filename, data, delimiter=",")
        with open('datasets/IHDP_sens/columns.txt', "w") as f:
            f.write(columns)
            f.write('\n\nx1-xN; first %i continuous features, then %i binary features' % (n_contfeats, n_binfeats))

    print('Done, %i generated data files.' % replications)
